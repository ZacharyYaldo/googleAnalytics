{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import datetime\n",
    "import gc\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "from pandas.io.json import json_normalize\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#csv_path differs from paul's original code, just modify if not running in kaggle kernel\n",
    "\n",
    "def load_df(csv_path='../input/train.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b5165c05b3d47251939839a2591a1b9806bf438",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining time predictors - Added \n",
    "\n",
    "def add_time_features(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='ignore')\n",
    "    df['year'] = df['date'].apply(lambda x: x.year)\n",
    "    df['month'] = df['date'].apply(lambda x: x.month)\n",
    "    df['day'] = df['date'].apply(lambda x: x.day)\n",
    "    df['weekday'] = df['date'].apply(lambda x: x.weekday())\n",
    "    df['visitStartTime_'] = pd.to_datetime(df['visitStartTime'],unit=\"s\")\n",
    "    df['visitStartTime_year'] = df['visitStartTime_'].apply(lambda x: x.year)\n",
    "    df['visitStartTime_month'] = df['visitStartTime_'].apply(lambda x: x.month)\n",
    "    df['visitStartTime_day'] = df['visitStartTime_'].apply(lambda x: x.day)\n",
    "    df['visitStartTime_weekday'] = df['visitStartTime_'].apply(lambda x: x.weekday())\n",
    "    return df\n",
    "date_features = [#\"year\",\"month\",\"day\",\"weekday\",'visitStartTime_year',\n",
    "    \"visitStartTime_month\",\"visitStartTime_day\",\"visitStartTime_weekday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cff67ff00772227c394c8de512cf4dd737d44b02",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = load_df(\"../input/train_v2.csv\")\n",
    "test = load_df(\"../input/test_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c5a8d6636edde8057e037b6cd6ab9ba5c288c53",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in test.columns:\n",
    "    if len(test[col].value_counts()) == 1:\n",
    "        test.drop(col,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb7cddc5107cb66e283a284f8f9416f465561b68",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if len(train[col].value_counts()) == 1:\n",
    "        train.drop(col,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d81ffae1cff49635ce4b4a7017c20b47df63985",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DID NOT REMOVE(NEW)\n",
    "test = test[test.columns.drop(list(test.filter(regex='trafficSource.adwordsClickInfo')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37fbac6ac13bce31630baaa59181ecd7a4625944",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DID NOT REMOVE(NEW)\n",
    "train = train[train.columns.drop(list(train.filter(regex='trafficSource.adwordsClickInfo')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "393c50136e2068e87f84b1e126cb428c96035abb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_col = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "889cce636819130a4f10486c00f7e2bbb5eb9086",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FILL NA (NEW)\n",
    "\n",
    "for col in num_col:\n",
    "    train[col] = train[col].fillna(\"0\").astype(\"int32\")\n",
    "    test[col] = test[col].fillna(\"0\").astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d876ead09de2373a3b6e419e227575ab246be937",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####FEATURE ENGINEERING (NEW)\n",
    "\n",
    "new_features = [\"hits_per_pageviews\"]     #combining hits and pageviews (see function below)\n",
    "new_category_features = [\"is_high_hits\"]\n",
    "def feature_engineering(df):\n",
    "    line = 4\n",
    "    df['hits_per_pageviews'] = (df[\"totals.hits\"]/(df[\"totals.pageviews\"])).apply(lambda x: 0 if np.isinf(x) else x)\n",
    "    df['is_high_hits'] = np.logical_or(train[\"totals.hits\"]>line,train[\"totals.pageviews\"]>line).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cfc094f9d92b19eb367c09ad2e14e0f162ee873",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#applying change, as well as time features (NEW)\n",
    "\n",
    "feature_engineering(train)\n",
    "feature_engineering(test)\n",
    "add_time_features(train)\n",
    "_ = add_time_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2f17e5b39133c09ee7479610768f37840fced55",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining categorical features, target, and useless features (NEW)\n",
    "\n",
    "category_features = [\"channelGrouping\", \"device.browser\", \n",
    "            \"device.deviceCategory\", \"device.operatingSystem\", \n",
    "            \"geoNetwork.city\", \"geoNetwork.continent\", \n",
    "            \"geoNetwork.subContinent\",\n",
    "            \"trafficSource.medium\", \n",
    "            \"trafficSource.source\",\n",
    "            ] + date_features\n",
    "target = 'totals.transactionRevenue'\n",
    "useless_col = [\"trafficSource.adContent\",                                                ##useless based on lgb feature importance/unique values/amount of NA\n",
    "              \"trafficSource.adwordsClickInfo.adNetworkType\", \n",
    "              \"trafficSource.adwordsClickInfo.page\",\n",
    "              \"trafficSource.adwordsClickInfo.slot\",\n",
    "              \"trafficSource.campaign\",\n",
    "              \"trafficSource.referralPath\",\n",
    "              'trafficSource.adwordsClickInfo.isVideoAd',\n",
    "              \"trafficSource.adwordsClickInfo.gclId\",\n",
    "              \"trafficSource.keyword\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ad76fa1b44339cd66e0a962da712f251a3b859f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excluded_features = [\n",
    "    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n",
    "    'visitId', 'visitStartTime'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    _f for _f in train.columns\n",
    "    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62849e846cd2443c4f104f2140be6a4c664309b2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in categorical_features:\n",
    "    train[f], indexer = pd.factorize(train[f])\n",
    "    test[f] = indexer.get_indexer(test[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2928703f4e8a1e00f1919a8b48637e40889c5b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inputting value for na's (NEW)\n",
    "train[target] = train[target].fillna(\"0\").astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c206eb96dcf67b4b8e0e091691e946a7ef6247fa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visit24(myTime):\n",
    "    visitCount=[]\n",
    "    for time in myTime:\n",
    "        visitBool = (myTime > time-pd.Timedelta('24 hours')) & (myTime < time)\n",
    "        visitCount.append(visitBool.sum())\n",
    "    return(pd.Series(visitCount))\n",
    "\n",
    "def visit48(myTime):\n",
    "    visitCount=[]\n",
    "    for time in myTime:\n",
    "        visitBool = (myTime > time-pd.Timedelta('48 hours')) & (myTime < time)\n",
    "        visitCount.append(visitBool.sum())\n",
    "    return(pd.Series(visitCount))\n",
    "\n",
    "def visit72(myTime):\n",
    "    visitCount=[]\n",
    "    for time in myTime:\n",
    "        visitBool = (myTime > time-pd.Timedelta('72 hours')) & (myTime < time)\n",
    "        visitCount.append(visitBool.sum())\n",
    "    return(pd.Series(visitCount))\n",
    "\n",
    "def visit168(myTime):\n",
    "    visitCount=[]\n",
    "    for time in myTime:\n",
    "        visitBool = (myTime > time-pd.Timedelta('168 hours')) & (myTime < time)\n",
    "        visitCount.append(visitBool.sum())\n",
    "    return(pd.Series(visitCount))\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['visitTotal'] = df.groupby('fullVisitorId')['date'].transform('size')\n",
    "    #df['v24']=df.groupby('fullVisitorId')['date'].transform(visit24)\n",
    "    #df['v48']=df.groupby('fullVisitorId')['date'].transform(visit48)\n",
    "    #df['v72']=df.groupby('fullVisitorId')['date'].transform(visit72)\n",
    "    df['v168']=df.groupby('fullVisitorId')['date'].transform(visit168)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "b953f76f392135b84eda727ce229703c158bf15d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'category_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-96085f7dc89a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#taking cleaned data from paul's code and modifying features to improve relevancy (NEW)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mall_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategory_features\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnum_col\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnew_category_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'category_features' is not defined"
     ]
    }
   ],
   "source": [
    "#taking cleaned data from paul's code and modifying features to improve relevancy (NEW)\n",
    "all_features = category_features+num_col+new_features+new_category_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe9bd338c360c2d6dea7fd2971932f2defac02e6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare for lgb\n",
    "\n",
    "train_x = train[all_features]\n",
    "train_y = train[target]\n",
    "test_x = test[all_features]\n",
    "for col in category_features:\n",
    "    print(\"transform column {}\".format(col))\n",
    "    lbe = LabelEncoder()\n",
    "    lbe.fit(pd.concat([train[col],test_x[col]]).astype(\"str\"))\n",
    "    train_x[col] = lbe.transform(train_x[col].astype(\"str\"))\n",
    "    test_x[col] = lbe.transform(test_x[col].astype(\"str\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d219a365ea39ef9add7c307c36d11846882d222",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining functions\n",
    "\n",
    "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples,learning_rate):\n",
    "    params = {\n",
    "    \"objective\" : \"regression\",\n",
    "    \"metric\" : \"rmse\", \n",
    "    \"num_leaves\" : int(num_leaves),\n",
    "    \"max_depth\" : int(max_depth),\n",
    "    \"lambda_l2\" : lambda_l2,\n",
    "    \"lambda_l1\" : lambda_l1,\n",
    "    \"num_threads\" : 4,\n",
    "    \"min_child_samples\" : int(min_child_samples),\n",
    "    \"learning_rate\" : learning_rate,\n",
    "    \"subsample_freq\" : 5,\n",
    "    \"bagging_seed\" : 42,\n",
    "    \"verbosity\" : -1\n",
    "    }\n",
    "    lgtrain = lgb.Dataset(train_x, label=np.log1p(train_y.apply(lambda x : 0 if x < 0 else x)),categorical_feature=category_features)\n",
    "    cv_result = lgb.cv(params,\n",
    "                       lgtrain,\n",
    "                       10000,\n",
    "                       categorical_feature=category_features,\n",
    "                       early_stopping_rounds=100,\n",
    "                       stratified=False,\n",
    "                       nfold=5)\n",
    "    return -cv_result['rmse-mean'][-1]\n",
    "\n",
    "def lgb_train(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, learning_rate):\n",
    "    params = {\n",
    "    \"objective\" : \"regression\",\n",
    "    \"metric\" : \"rmse\", \n",
    "    \"num_leaves\" : int(num_leaves),\n",
    "    \"max_depth\" : int(max_depth),\n",
    "    \"lambda_l2\" : lambda_l2,\n",
    "    \"lambda_l1\" : lambda_l1,\n",
    "    \"num_threads\" : 4,\n",
    "    \"min_child_samples\" : int(min_child_samples),\n",
    "    \"learning_rate\" : learning_rate,\n",
    "    \"subsample_freq\" : 5,\n",
    "    \"bagging_seed\" : 42,\n",
    "    \"verbosity\" : -1\n",
    "    }\n",
    "    t_x,v_x,t_y,v_y = train_test_split(train_x,train_y,test_size=0.2)\n",
    "    lgtrain = lgb.Dataset(t_x, label=np.log1p(t_y.apply(lambda x : 0 if x < 0 else x)),categorical_feature=category_features)\n",
    "    lgvalid = lgb.Dataset(v_x, label=np.log1p(v_y.apply(lambda x : 0 if x < 0 else x)),categorical_feature=category_features)\n",
    "    model = lgb.train(params, lgtrain, 2000, valid_sets=[lgvalid], early_stopping_rounds=100, verbose_eval=100)\n",
    "    pred_test_y = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba778fb218220d0a440cd877877f2c7139fe64e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#param_tuning defined above, (init_points, num_iter,**args)...utilizes bayesian optimization\n",
    "##defining bo ranges for parameter tuning    \n",
    "# def param_tuning(init_points,num_iter,**args):\n",
    "#     lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 50),\n",
    "#                                                 'max_depth': (5, 15),\n",
    "#                                                 'lambda_l2': (0.0, 0.05),\n",
    "#                                                 'lambda_l1': (0.0, 0.05),\n",
    "#                                                 'bagging_fraction': (0.5, 0.8),\n",
    "#                                                 'feature_fraction': (0.5, 0.8),\n",
    "#                                                 'min_child_samples': (20, 50),\n",
    "#                                                 })\n",
    "\n",
    "#     lgbBO.maximize(init_points=init_points, n_iter=num_iter,**args)\n",
    "#     return lgbBO\n",
    "# result = param_tuning(5,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############ADDED USER LEVEL############# Based on Kernel Example https://www.kaggle.com/scirpus/feature-fest-with-genetic-programming\n",
    "\n",
    "#did not run user level in lgb due to kernel issue, this code should work\n",
    "\n",
    "train['predictions'] = np.expm1(np.zeros(train.shape[0]))\n",
    "test['predictions'] = np.zeros(test.shape[0])\n",
    "\n",
    "# Aggregate data at User level\n",
    "trn_data = train[all_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "\n",
    "# Create multiple predictions for each user\n",
    "trn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.predictions))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "    \n",
    "\n",
    "# Create a DataFrame with VisitorId as index\n",
    "# trn_pred_list contains dictionary so creating a dataframe with this will expand dict values into columns\n",
    "trn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\n",
    "trn_feats = trn_all_predictions.columns\n",
    "trn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\n",
    "trn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\n",
    "trn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\n",
    "trn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\n",
    "trn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)\n",
    "full_data = pd.concat([trn_data, trn_all_predictions], axis=1)\n",
    "del trn_data, trn_all_predictions\n",
    "gc.collect()\n",
    "full_data.shape\n",
    "\n",
    "\n",
    "sub_pred_list = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.predictions))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "\n",
    "##calculating groupby by mean values\n",
    "sub_data = test[all_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "sub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\n",
    "for f in trn_feats:\n",
    "    if f not in sub_all_predictions.columns:\n",
    "        sub_all_predictions[f] = np.nan\n",
    "sub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\n",
    "sub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\n",
    "sub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\n",
    "sub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\n",
    "sub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\n",
    "sub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\n",
    "del sub_data, sub_all_predictions\n",
    "gc.collect()\n",
    "sub_full_data.shape\n",
    "\n",
    "\n",
    "\n",
    "##create user level target\n",
    "train['target'] = train['totals.transactionRevenue'].fillna(0)\n",
    "trn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()\n",
    "\n",
    "\n",
    "df = pd.concat([full_data,sub_full_data],sort=False)\n",
    "trainsize = full_data.shape[0]\n",
    "del full_data, sub_full_data\n",
    "gc.collect()\n",
    "df = df.reset_index(drop=False)\n",
    "for c in df.columns[1:]:\n",
    "    if((df[c].min()>=0)&(df[c].max()>=10)):\n",
    "        df[c] = np.log1p(df[c])\n",
    "    elif((df[c].min()<0)&((df[c].max()-df[c].min())>=10)):\n",
    "        df.loc[df[c]!=0,c] = np.sign(df.loc[df[c]!=0,c])*np.log(np.abs(df.loc[df[c]!=0,c]))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for c in df.columns[1:]:\n",
    "    ss = StandardScaler()\n",
    "    df.loc[~np.isfinite(df[c]),c] = np.nan\n",
    "    df.loc[~df[c].isnull(),c] = ss.fit_transform(df.loc[~df[c].isnull(),c].values.reshape(-1,1))\n",
    "df.fillna(-99999,inplace=True)\n",
    "gp_trn_users = df[:trainsize].copy().set_index('fullVisitorId')\n",
    "maxval = np.log1p(trn_user_target['target'].values).max()\n",
    "gp_trn_users['target'] = np.log1p(trn_user_target['target'].values)\n",
    "gp_trn_users.target /= maxval\n",
    "gp_sub_users = df[trainsize:].copy().set_index('fullVisitorId')\n",
    "newcols =  [x.replace('.','_') for x in gp_trn_users.columns]\n",
    "gp_trn_users.columns = newcols\n",
    "newcols =  [x.replace('.','_') for x in gp_sub_users.columns]\n",
    "gp_sub_users.columns = newcols\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c5250df800ebbb08793ab295991df87f69ed2f2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_tuning_boundaryShift(init_points,num_iter,**args):\n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (20, 40),\n",
    "                                                'max_depth': (14, 30),\n",
    "                                                'lambda_l2': (0.0, 0.05),\n",
    "                                                'lambda_l1': (0.0, 0.05),\n",
    "                                                'min_child_samples': (15, 40), 'learning_rate': (0.01, 0.04)                                            \n",
    "                                                })\n",
    "\n",
    "    lgbBO.maximize(init_points=init_points, n_iter=num_iter,**args)\n",
    "    return lgbBO\n",
    "result = param_tuning_boundaryShift(20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4349f63d7fa94b4d372cf40d096cb6da3dc1c8a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.res['max']['max_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "443c5d65fe174d8a6b29aba48bf6087ce11ff230",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred1,model1 = lgb_train(**result.res['max']['max_params'])\n",
    "pred2,model2 = lgb_train(**result.res['max']['max_params'])\n",
    "pred3,model3 = lgb_train(**result.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2b29cc358b7e41e4fc4307ca611dead13a9579e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
